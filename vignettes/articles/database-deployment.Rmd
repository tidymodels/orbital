---
title: "Database deployment"
---

```{r}
#| include: false
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  message = FALSE
)
```

Once you've created an orbital object, you can deploy it to a database by creating a TABLE or VIEW. Both approaches use the same underlying SQL generation, but they have different tradeoffs:

- **Tables** store pre-computed predictions. They're fast to query and work well for large datasets or complex models. The tradeoff is that predictions become stale when data changes, so you'll need a scheduled job to refresh them.

- **Views** compute predictions on-the-fly each time they're queried. Predictions are always fresh, but query performance depends on model complexity and data size. Views are useful for prototyping, smaller datasets, or when you need real-time predictions.

## Setup

We start by loading our packages and creating a simple fitted workflow.

```{r}
#| label: setup
library(orbital)
library(recipes)
library(parsnip)
library(workflows)
library(dplyr)
library(DBI)
library(duckdb)
```

```{r}
rec_spec <- recipe(mpg ~ disp + wt + hp, data = mtcars) |>
  step_normalize(all_numeric_predictors())

lm_spec <- linear_reg()

wf_spec <- workflow(rec_spec, lm_spec)
wf_fit <- fit(wf_spec, data = mtcars)
```

Then create our orbital object.

```{r}
orbital_obj <- orbital(wf_fit)
orbital_obj
```

## Connect to a database

We'll use DuckDB as our example database since it runs in-memory and requires no external setup. The same pattern works with other databases like PostgreSQL, Snowflake, SQL Server, and Spark.

```{r}
con <- dbConnect(duckdb(dbdir = ":memory:"))
mtcars_db <- copy_to(con, mtcars, name = "mtcars_data")
```

## Generating the prediction SQL

Both tables and views start the same way: use `orbital_inline()` with `dplyr::mutate()` to apply predictions to a database table, then extract the generated SQL.

```{r}
# Apply predictions to the table
predictions <- mtcars_db |>
  mutate(!!!orbital_inline(orbital_obj))

# View the lazy query
predictions
```

We can extract the SQL query using `dbplyr::remote_query()`.

```{r}
library(dbplyr)
generated_sql <- remote_query(predictions)
generated_sql
```

## Creating a table

Tables store predictions at a point in time. They're fast to query and work well with large datasets or complex models.

```{r}
table_name <- "mtcars_predictions"
table_sql <- paste("CREATE OR REPLACE TABLE", table_name, "AS", generated_sql)
dbExecute(con, table_sql)
```

```{r}
tbl(con, table_name) |>
  collect()
```

The table contains predictions computed at the time it was created. To keep predictions fresh, you would schedule a job (e.g., daily or hourly) to drop and recreate the table, or use an incremental update strategy that only scores new rows.

## Creating a view

Views compute predictions on-the-fly, providing always-fresh results without needing a refresh job.

```{r}
view_name <- "mtcars_predictions_view"
view_sql <- paste("CREATE OR REPLACE VIEW", view_name, "AS", generated_sql)
dbExecute(con, view_sql)
```

```{r}
tbl(con, view_name) |>
  collect()
```

Keep in mind that the prediction SQL runs every time the view is queried. For complex models or large tables, this can be slow.

## Selecting specific columns

In production, you often want to include only the prediction column and an identifier column, rather than all the intermediate calculations.

```{r}
# Select only ID-like columns and the prediction
predictions_slim <- mtcars_db |>
  mutate(row_id = row_number(), !!!orbital_inline(orbital_obj)) |>
  select(row_id, .pred)

slim_sql <- remote_query(predictions_slim)

table_sql <- paste("CREATE OR REPLACE TABLE mtcars_pred_slim AS", slim_sql)
dbExecute(con, table_sql)

tbl(con, "mtcars_pred_slim") |>
  collect()
```

## Database-specific considerations

Consider versioning your prediction tables or views (e.g., `model_v1`, `model_v2`) so you can compare predictions across model versions or roll back if needed.

The pattern shown above works across most SQL databases, but there are some differences to be aware of:

- **Column naming**: Some databases (e.g., Databricks) don't allow column names with dots. Use `orbital(wf_fit, prefix = "pred")` to generate columns named `pred` instead of `.pred`.
- **SQL Server**: Uses `CREATE OR ALTER VIEW` instead of `CREATE OR REPLACE VIEW` (requires SQL Server 2016 or later).
- **SQLite**: Uses `DROP VIEW IF EXISTS` followed by `CREATE VIEW` since it doesn't support `CREATE OR REPLACE`.
- **Function support**: Complex models using functions like `log()`, `exp()`, or probability functions may behave differently across databases. Always test your generated SQL on your target database.
- **Query complexity**: Very large models (e.g., tree ensembles with many trees) may generate SQL that exceeds database-specific limits on expression depth or query length.

This article covers the basics. Production deployments often involve additional considerations like access controls, monitoring, logging, and integration with your organization's data infrastructure.

```{r}
#| include: false
dbDisconnect(con)
```
